{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0.dev20220721+cu113'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FSDP currently does not support layer level fine tuning.  Thus options are whole model fine tuning, \n",
    "or for large language models, Child Tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Child Tuning was developed in the paper: \n",
    "Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@inproceedings{xu-etal-2021-childtuning,\n",
    "    title = \"Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning\",\n",
    "    author = \"Runxin Xu and\n",
    "    Fuli Luo and Zhiyuan Zhang and\n",
    "    Chuanqi Tan and Baobao Chang and\n",
    "    Songfang Huang and Fei Huang\",\n",
    "    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n",
    "    year = \"2021\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/2109.05687"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Paper intro](./images/child_tuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Improved results](./images/child_tuning_gains.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2022)\n",
    "torch.cuda.manual_seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = torch.randn(3,3)\n",
    "reserve_p = .30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0203,  0.1361, -0.9314],\n",
       "        [ 1.3920,  0.7097, -2.1463],\n",
       "        [ 0.9796,  0.2208, -0.3193]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3000, 0.3000, 0.3000],\n",
       "        [0.3000, 0.3000, 0.3000],\n",
       "        [0.3000, 0.3000, 0.3000]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = grad.new_full(size=grad.size(), fill_value=reserve_p)\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bernoulli(probs: torch.Size([3, 3]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdist = Bernoulli(r)\n",
    "rdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp = rdist.sample() \n",
    "rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 3.3333, 0.0000],\n",
       "        [0.0000, 0.0000, 3.3333],\n",
       "        [3.3333, 0.0000, 3.3333]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amplifier = rp/reserve_p\n",
    "amplifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.1021,  0.0000],\n",
       "        [ 0.0000, -0.0000, -3.0270],\n",
       "        [-7.7042,  0.0000,  6.8715]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newgrad = grad*amplifier\n",
    "newgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_mask = Bernoulli(grad.new_full(size=grad.size(), fill_value=reserve_p))\n",
    "grad *= grad_mask.sample() / reserve_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two versions of child Tuning - task free and task dependent. In T5 testing, had better results with Task Free, so that's what we'll show here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(for reference - Task dependent = you train one epoch with the parameters being monitored to create a Fisher Information Matrix, or the most 'active' parameters for that task.  These are then isolated and the only ones updated during fine tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task free is more akin to a strong regularizer due to the random masking of a subset of the model params.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage:\n",
    "\n",
    "from ChildTuningOptimizer import ChildTuningAdamW\n",
    "\n",
    "model = torch.nn.Linear(100,200) # lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel as FSDP,\n",
    "    MixedPrecision,\n",
    "    StateDictType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- main FSDP init -----------\n",
    "    model = FSDP(\n",
    "        model,\n",
    "        auto_wrap_policy=my_auto_wrap_policy,\n",
    "        mixed_precision=mp_policy,\n",
    "        backward_prefetch=prefetch_policy,\n",
    "        sharding_strategy=cfg.sharding_strategy,\n",
    "        device_id=torch.cuda.current_device(),\n",
    "        forward_prefetch=cfg.forward_prefetch,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ChildTuningAdamW(model.parameters(), lr=4e-8, reserve_p=0.35, mode=\"taskfree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Child Tuning otherwise works same as AdamW, but with the masked tuning.  Provides finer grained tuning vs the hard layer freezing as it operates both horizontally and vertically within the entire model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - during Child Fine Tuning, you can adjust the reserve_p percentage.  Percentage of 1.0 = normal AdamW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General best practice is around 30% - 35% of the network should be used for the fine tuning task (ala reserve_p = .30 - .35), but you can run / compare for your specific task.   \n",
    "Child Tuning will often lag vs 'whole model' for the first few epochs, but then will usually catch up and exceed after that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch_p39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb71ac050f92a5d5e3cdea462f22e174d379ee0836c3076b1e7df4a375a19e34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
