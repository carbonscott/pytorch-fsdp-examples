{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0.dev20220711+cu113'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FSDP Mixed Precision supports BFloat16 and FP16 with fine grained policies that control paramaters, gradient communications and buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Mixed Precision class along with FSDP import:\n",
    "\n",
    "from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel as FSDP,\n",
    "    MixedPrecision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a policy - one for Bfloat16 is shown:\n",
    "\n",
    "bfloatPolicy = MixedPrecision(\n",
    "        # Param precision\n",
    "        param_dtype=torch.bfloat16,\n",
    "        # Gradient communication precision.\n",
    "        reduce_dtype=torch.bfloat16,\n",
    "        \n",
    "    )\n",
    "\n",
    "# you can mix types:\n",
    "comboPolicy = MixedPrecision(\n",
    "        # Param precision\n",
    "        param_dtype=torch.bfloat16,\n",
    "        # Gradient communication precision.\n",
    "        reduce_dtype=torch.float32,\n",
    "        # Buffer precision.\n",
    "        buffer_dtype=torch.float32,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then simply pass the policy in during FSDP init:\n",
    "# ----- main FSDP init -----------\n",
    "model = FSDP(\n",
    "        model,\n",
    "        auto_wrap_policy=my_auto_wrap_policy,\n",
    "\n",
    "        mixed_precision=bfloatPolicy,    #  < --------- mixed precision policy\n",
    "        \n",
    "        backward_prefetch=prefetch_policy,\n",
    "        sharding_strategy=cfg.sharding_strategy,\n",
    "        device_id=torch.cuda.current_device(),\n",
    "        forward_prefetch=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bfloat offers significant speedup of training - nearly 2x, can go higher based on memory\n",
    "![val loss comparison](./bfloat_training.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### refresher on the mixed precision types\n",
    "![](./datatypes_mp.png)\n",
    "Image credit: Nvidia - https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details:\n",
    "\n",
    "# 1 - BatchNorm is automatically kept in fp32 for precision (overrides buffer policy, no user action needed)\n",
    "# 2 - Local gradients during backprop are also always fp32 (automatic, no user action needed)\n",
    "# 3 - Models are always saved in fp32 format for max portability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bfloat16 support verification imports (network and gpu native support)\n",
    "from pkg_resources import packaging\n",
    "import torch.cuda.nccl as nccl\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_bfloat_support = (\n",
    "    torch.version.cuda\n",
    "    and torch.cuda.is_bf16_supported()\n",
    "    and packaging.version.parse(torch.version.cuda).release >= (11, 0)\n",
    "    and dist.is_nccl_available()\n",
    "    and nccl.version() >= (2, 10)\n",
    ")\n",
    "\n",
    "# simple check =\n",
    "basic_bfloat_ready = torch.cuda.is_bf16_supported()   # does not confirm network can handle it, just gpu native support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important point - always verify native bfloat support is available!  \n",
    "V100 GPU's will 'support' bfloat if you just run without checking, but its emulated \n",
    "and training will run much, much slower (worse than fp32!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_bfloat_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp16 requires not just adding a policy...requires the sharded grad scaler:\n",
    "if cfg.use_fp16:\n",
    "        from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler\n",
    "        scaler = ShardedGradScaler()\n",
    "\n",
    "\n",
    "# in training loop:\n",
    "loss = output[\"loss\"]\n",
    "if scaler:\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()  # adjust scaling for next minibatch\n",
    "    else:\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend using BFloat16 if possible.  \n",
    "# FP16 runs 4% slower vs Bfloat16, all things equal, likely due to cost of rescaling. \n",
    "# Rescaler has to play guessing game of how much to rescale, \n",
    "# bad guesses mean that mini-batch is tossed due to having NAN values (inefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf32 can be used as well, but is not controlled atm via FSDP policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The flag below controls whether to allow TF32 on matrix multiplies. This flag defaults to False\n",
    "# in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# not as fast as BFloat16, but faster than FP32 (10 bits for precision vs 7 for Bfloat)\n",
    "# Even Nvidia notes bfloat is faster - \n",
    "# \"For maximum performance, the A100 also has enhanced 16-bit math capabilities.  \n",
    "# It supports both FP16 and Bfloat16 (BF16) at double the rate of TF32. \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch_p39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb71ac050f92a5d5e3cdea462f22e174d379ee0836c3076b1e7df4a375a19e34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
