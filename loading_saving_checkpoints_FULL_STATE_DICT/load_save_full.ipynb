{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FSDP - Saving and Loading Models and Optimizer checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial covers loading and saving via FULL_STATE_DICT, or where the model and optimizer are fully assembled on the rank 0 cpu memory.  This means there is an upper limit of model sizes that can be saved based on cpu memory...assume models around 20B will fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger models, we'll use LOCAL_STATE_DICT, which will be covered seperately.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0.dev20220627+cu113'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import FSDP with two additional items:\n",
    "from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel as FSDP,\n",
    "    CPUOffload,\n",
    "    MixedPrecision,\n",
    "    BackwardPrefetch,\n",
    "    ShardingStrategy,\n",
    "    FullStateDictConfig,  # < -- configuration policy for full_state actions\n",
    "    StateDictType,  # < -- enum, use to confirm what type of states we are handling (in this case FULL_STATE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "model = build_model(cfg.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint for model\n",
    "# preload checkpoint if desired\n",
    "    if (\n",
    "        cfg.load_model_checkpoint\n",
    "        and cfg.checkpoint_type == StateDictType.FULL_STATE_DICT\n",
    "    ):\n",
    "        model_checkpointing.load_model_checkpoint(model, rank, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load function is just like regular PyTorch checkpoint loading for FULL_STATE\n",
    "# only called on Rank0!\n",
    "def load_model_checkpoint(model, rank, cfg, verbose=True):\n",
    "    \"\"\"load local checkpoint to rank0 cpu\n",
    "    must be called * before * passing to FSDP\"\"\"\n",
    "\n",
    "    if rank != 0:\n",
    "        return\n",
    "\n",
    "    # where is the checkpoint at...\n",
    "    full_state_dict_model_path = (\n",
    "        Path.cwd() / cfg.checkpoint_folder / cfg.checkpoint_model_filename\n",
    "    )\n",
    "    # is it present...\n",
    "    if not full_state_dict_model_path.is_file():\n",
    "        print(\n",
    "            f\"model checkpoint {full_state_dict_model_path} not present. Returning...\"\n",
    "        )\n",
    "        return\n",
    "        \n",
    "    # load the checkpoint\n",
    "    model_checkpoint = torch.load(full_state_dict_model_path)\n",
    "    # integrate into loaded model\n",
    "    model.load_state_dict(model_checkpoint)\n",
    "\n",
    "    if cfg.verbose:\n",
    "        print(f\"model checkpoint loaded to rank0 cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init FSDP and shard the model\n",
    "# ----- main FSDP init -----------\n",
    "    model = FSDP(\n",
    "        model,\n",
    "        auto_wrap_policy=my_auto_wrap_policy,\n",
    "        mixed_precision=mp_policy,\n",
    "        # backward_prefetch=prefetch_policy,\n",
    "        device_id=torch.cuda.current_device(),\n",
    "        sharding_strategy=ShardingStrategy.FULL_SHARD,  # Zero2\n",
    "        # cpu_offload= cpu_policy,\n",
    "        forward_prefetch=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare optimizer with sharded model\n",
    "# optimizer ----------\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=8e-4, weight_decay=0.005\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimizer checkpoint\n",
    "    if cfg.load_optimizer:\n",
    "        model_checkpointing.load_optimizer_checkpoint(model, optimizer, rank, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_optimizer_checkpoint(model, optimizer, rank, cfg):\n",
    "    \"\"\"load an fdsp optimizer full_state checkpoint using scatter method\n",
    "    this ensures only rank 0 loads the optimizer state dict and scatters to other ranks\"\"\"\n",
    "\n",
    "    opt_file_path = Path.cwd() / cfg.checkpoint_folder / cfg.optimizer_checkpoint_file\n",
    "\n",
    "    if not opt_file_path.is_file():\n",
    "        print(\n",
    "            f\"warning - optimizer checkpoint not present {opt_file_path}. Returning. \"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    full_osd = None\n",
    "\n",
    "    if rank == 0:\n",
    "        full_osd = torch.load(opt_file_path)\n",
    "\n",
    "        if cfg.verbose:\n",
    "            print(f\"loaded full osd on rank 0\")\n",
    "\n",
    "    # called from all ranks, though only rank0 has a valid param for full_osd\n",
    "    sharded_osd = FSDP.scatter_full_optim_state_dict(full_osd, model)\n",
    "\n",
    "    if cfg.verbose:\n",
    "        print(f\"optimizer shard loaded on rank {rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check metrics, decided to save model and optimizer\n",
    "# model checkpointing\n",
    "\n",
    "# create singleton saving policies to avoid making over and over\n",
    "fullstate_save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_checkpoint(\n",
    "    model,\n",
    "    optimizer,\n",
    "    rank,\n",
    "    cfg,\n",
    "    epoch=1,\n",
    "):\n",
    "    \"\"\"saving model via rank0 cpu streaming and full_state_dict\"\"\"\n",
    "\n",
    "    # saving with rank0 cpu\n",
    "    if not cfg.checkpoint_type == StateDictType.FULL_STATE_DICT:\n",
    "        print(f\" unable to handle checkpoint type {cfg.checkpoint_type}, aborting\")\n",
    "\n",
    "    with FSDP.state_dict_type(\n",
    "        model, StateDictType.FULL_STATE_DICT, fullstate_save_policy\n",
    "    ):\n",
    "        cpu_state = model.state_dict()\n",
    "\n",
    "    if cfg.verbose:\n",
    "        print(f\"saving process: rank {rank}  done w model state_dict\")\n",
    "\n",
    "    if rank == 0:\n",
    "        print(f\"--> saving model ...\")\n",
    "        # create save path\n",
    "        save_dir = Path.cwd() / cfg.checkpoint_folder\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        save_name = cfg.model_save_name + \"-\" + str(epoch) + \".pt\"\n",
    "        save_full_path = str(save_dir) + \"/\" + save_name\n",
    "\n",
    "        # save model\n",
    "        torch.save(cpu_state, save_full_path)\n",
    "\n",
    "        if cfg.verbose:\n",
    "            print(f\"model checkpoint saved for epoch {epoch} at {save_full_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and saving our optimizer\n",
    "def save_optimizer_checkpoint(model, optimizer, rank, cfg, epoch=1):\n",
    "    \"\"\"save optimizer state via full state dict\"\"\"\n",
    "\n",
    "    if cfg.verbose:\n",
    "        print(f\"--> optim state call on rank {rank}\")\n",
    "\n",
    "    # pull all sharded optimizer states to rank0 cpu...\n",
    "\n",
    "    optim_state = FSDP.full_optim_state_dict(model, optimizer)\n",
    "\n",
    "    if cfg.verbose:\n",
    "        print(f\"optim state dict ready on {rank} and len of {len(optim_state)}\")\n",
    "\n",
    "    if rank == 0:\n",
    "        save_dir = Path.cwd() / cfg.checkpoint_folder\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        opt_save_name = (\n",
    "            cfg.optimizer_name + \"-\" + cfg.model_save_name + \"-\" + str(epoch) + \".pt\"\n",
    "        )\n",
    "        opt_save_full_path = save_dir / opt_save_name\n",
    "\n",
    "        # note that saving can be time consuming...i.e. 1.5B can take up to 3 minutes (17GB)\n",
    "        # thus always print state so no one thinks it has hung\n",
    "        print(f\"--> saving optimizer state...\")\n",
    "\n",
    "        torch.save(optim_state, opt_save_full_path)\n",
    "\n",
    "        print(f\"--> saved {opt_save_full_path} to disk\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch_p39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb71ac050f92a5d5e3cdea462f22e174d379ee0836c3076b1e7df4a375a19e34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
