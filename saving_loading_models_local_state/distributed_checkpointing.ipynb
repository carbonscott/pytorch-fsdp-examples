{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0.dev20220719+cu113'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed checkpoints, also known as local state dict saving! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike full_state_dict, saving with a distributed checkpoint will not create a single .pt file...rather it will generate hundreds to thousands of smaller files all within a directory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows for saving of gigantic models that would otherwise exceed CPU memory.  Full state dict will try to assemble the whole model in CPU memory, vs distributed checkpointing will not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel as FSDP,\n",
    "    StateDictType,\n",
    "    FullStateDictConfig,  # general model non-sharded, non-flattened params\n",
    "    LocalStateDictConfig,  # flattened params, usable only by FSDP\n",
    "    # ShardedStateDictConfig, # un-flattened param but shards, usable by other parallel schemes.\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed._shard.checkpoint import (\n",
    "    FileSystemReader,\n",
    "    FileSystemWriter,\n",
    "    save_state_dict,\n",
    "    load_state_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_distributed_model_checkpoint(model, rank, cfg):\n",
    "\n",
    "    if cfg.checkpoint_type == StateDictType.LOCAL_STATE_DICT:\n",
    "        print(f\"loading distributed checkpoint, rank {rank}...\")\n",
    "        folder_name = cfg.dist_checkpoint_root_folder+\"/\"+cfg.dist_checkpoint_folder+\"-\"+cfg.model_name\n",
    "\n",
    "        checkdir = Path.cwd() / folder_name\n",
    "\n",
    "        if not checkdir.exists():\n",
    "            if rank==0:\n",
    "                print(f\"No checkpoint directory found...skipping\")\n",
    "            return\n",
    "\n",
    "\n",
    "        reader = FileSystemReader(checkdir)\n",
    "\n",
    "        with FSDP.state_dict_type(\n",
    "            model,\n",
    "            StateDictType.LOCAL_STATE_DICT,\n",
    "        ):\n",
    "            state_dict = model.state_dict()\n",
    "            load_state_dict(state_dict, reader)\n",
    "            \n",
    "            model.load_state_dict(state_dict)\n",
    "\n",
    "        print(f\"--> local state loaded on rank {rank}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_distributed_model_checkpoint(model, rank, cfg, epoch=1):\n",
    "    # distributed checkpoint saving\n",
    "\n",
    "    if rank == 0:\n",
    "        print(f\"Starting distributed checkpoint save...\")\n",
    "    \n",
    "\n",
    "    # confirm type of checkpoint and save\n",
    "    if cfg.checkpoint_type == StateDictType.LOCAL_STATE_DICT:\n",
    "        # create writer to current path\n",
    "        #folder_name = cfg.dist_checkpoint_folder+\"-\"+cfg.model_name\n",
    "        folder_name = cfg.dist_checkpoint_root_folder+\"/\"+cfg.dist_checkpoint_folder+\"-\"+cfg.model_name\n",
    "        save_dir = Path.cwd() / folder_name\n",
    "\n",
    "        writer = FileSystemWriter(save_dir)\n",
    "\n",
    "        with FSDP.state_dict_type(\n",
    "            model,\n",
    "            StateDictType.LOCAL_STATE_DICT,\n",
    "        ):\n",
    "            state_dict = model.state_dict()\n",
    "\n",
    "        # write out distributed checkpoint\n",
    "        save_state_dict(state_dict, writer)\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"--> distributed checkpoint saved at {save_dir}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes - be very careful to increment your directory names for distributed checkpoints.  Saving into the same\n",
    "# directory will overwrite the previous .metadata file that controls the info on all the guid-named files within. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch_p39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb71ac050f92a5d5e3cdea462f22e174d379ee0836c3076b1e7df4a375a19e34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
